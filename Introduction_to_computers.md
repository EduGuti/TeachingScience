En principio voy a ir poniendo un temario para enseñar informática y electrónica desde cero.
 - Electrónica analógica y electrónica digital. ¿Cuál es la diferencia? ¿Sigue teniendo sentido seguir investigando/avanzando en electrónica analógica?
 - Definición de ordenador/computador: En principio partiendo del famoso artículo/paper de Alan Turing de 1936, ya que me parece que la principal referencia histórica (y desde un punto de vista teórico) al enseñar esto en el año en el que escribo esto (el 2023). Pero hay muchos otros personajes históricos que contribuyeron a crear las bases de la computación, o que lo hicieron de manera alternativa (como Charles Babbage, que escribió sobre esto más de 100 años antes que Turing, pero su trabajó quedó descontinuado).
 - Tipos de ordenadores: Digitales y analógicos.
   - Tipos de ordenadores digitales: en base 10, como el de Charles Babbage, o en base 2 (binarios), como los actuales.
   - Tipos de implementación física:
     - Desde un punto de vista histórico:
       - Con engranajes: La máquina analítica de Charles Babbage, que nunca llegó a ser terminada.
       - Con relés ("electromecánicos").
       - Con válvulas de vacío.
       - Con transistores: 1º sin circuitos integrados y finalmente con circuitos integrados (es decir, creados con obleas de silicio dopadas para crear circuitos (eléctricos) microscópicos).
       - Cuánticos.
     - Implementaciones alternativas:
       - Con fluidos: Tienen aplicaciones prácticas en sistemas/entornos en los que la electricidad es peligrosa, pero también hay implementaciones de este tipo que son interesantes desde un punto de vista didáctico.
       - Con fichas de dominó (puramente lucrativo y hay que tener muuuucha paciencia, jajaja).
       - Con átomos/moléculas.
 - Diseño de CPUs: Uso del simulador Logisim para enseñar el funcionamiento de CPUs básicas, incluyendo la creación de memorias y ALUs con puertas lógicas.
